### 神经网络类的一些辅助函数介绍

#### 预测某一个输入量

我们在前面已经完成了神经网络的算法实现以及训练的部分，但是我们怎么来权衡这个神经网络的优劣呢？我记得邓小平有一句话，是：实践是检验真理的唯一标准。的确，对于我们的神经网络，我们看它是否合理只有测试它预测的准确性了，对于一个未知标签的输入数据，如果神经网络的预测结果与期望结果相类似，那么这个神经网络无疑是比较好的，现在我们开始来实现这个预测的功能吧。

其实预测不过是我们让我们需要预测的数据在神经网络里面“走”一遍，也就是从输入层到输出层的一个过程，只不过现在我们的权值已经训练过了，所以我们已经不需要什么反馈的过程。

```bash
    def predict(self, a):
        """
            预测某一个数据的值，假如该网络的输入层神经元的个数是100个的话，
            则a为100行，1列的二维数组。
                inputs :
                    a : 用来测试的数据
                returns:
                    预测的结果
        """
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(numpy.dot(w, a)+b)
        return a
```

我们使用的神经元是sigmoid神经元，所以你在这个函数上看到的知识我们简单地调用了一个sigmoid函数，这个sigmoid函数不是类中的函数（当然，你也可以将其写进去），而`numpy.dot(w, a)+b`这个语句则是我们在前面的公式推导的时候的`z`。

```bash
def sigmoid(z):
    """
        激活函数
    """
    return 1.0 / ( 1.0 + numpy.exp(-z) )
```

### 获得某一个输入数据集的预测精度

对于单个的输入我们已经知道了如何得到最终的预测结果了，想一想我们前面的`SGD`函数里面的test数据集，我们是怎么得到整个数据集的测试精度的呢？首先我们循环测试这个数据集中的单个的预测结果，然后与期望输出的结果进行比较，最终得出测试的精度。

```bash
    def getAccuracy(self, data ):
        """
            获取某一个测试数据集的测试精度
            inumpyuts：
                data : 测试的数据集，与training_data是相类似的
            
            returns:
                测试集的正确率
        """
        #numpy.argmax() 返回某一个数组中最大的值的下标
        results = [( numpy.argmax( self.predict(x) ), \
                    numpy.argmax( y ) )
                    for (x, y) in data]
          
        #返回所有正确预测的数据的总和         
        return sum(int(x == y) for (x, y) in results)
```

其实这些代码并不难，知识有一些函数需要解释一下，所以我在那些需要解释的函数之前都写上了一些注释。


### 保存神经网络的参数

这个问题很重要，我想再次说明一下我为什么对这个问题“耿耿于怀”，因为我在使用opencv的ANN的时候竟然没有办法保存神经网络的权值，我问了百度以及谷歌，得出的结果是大家也是不知道怎么保存，这不得不说这是一个大坑啊。不过我仍然相信这opencv之中应该是会有保存的相关代码的，不过使用Python我不知道如何保存而已。所以在这里希望如果你对此颇有研究，请在github上issue上告诉我该怎么做，先谢谢了。不说废话了，想一想，我们的神经网络的参数应该包含哪些呢？我想应该是下面的几个：

+ 神经层的数量以及每一个神经层的神经元的个数

+ 每一个神经元的权值与阈值

差不多了吧，对于我们已经定义了的BPANN类来说，这些参数就是`size`、`weights`、`biases`，现在我们定义一下存储参数的方法，我们想将这些参数存储层json格式的（当然，你也可以存储成csv、txt之类的），我们需要引入`json`包，在我们的包引入的部分，加上下面一条语句：

```bash
import json
```

然后就看看这个函数吧！代码很简洁，不难看懂。

```bash
    def save(self, filename):
        """
            将神经网络的神经层的信息 、 权值、阈值三个参数保存到文件中。
            inumpyut :
                filename : 存储数据的文件
        """
        data = {"sizes": self.sizes,
                "weights": [w.tolist() for w in self.weights],
                "biases": [b.tolist() for b in self.biases]}
        f = open(filename, "w")
        json.dump(data, f)
        f.close()
```


#### 获取保存的神经网络的参数来初始化神经网络

我们保存那些参数干嘛？不就是为了我们不需要每次都花很长的时间来训练神经网络吗？现在来看一看整个的提取程序，在这里我没有将这个函数作为BPANN的内部函数。


```bash
def load(filename):
    """
        读取由save存储的数据，并初始化神经网络
        inumpyut :
            filename : 存储数据的文件名
        returns :
            net : 初始化之后的神经网络
    """
    f = open(filename, "r")
    data = json.load(f)
    f.close()
    net = BPANN( data["sizes"] )
    net.weights = [numpy.array(w) for w in data["weights"]]
    net.biases = [numpy.array(b) for b in data["biases"]]
    
    return net
```

这个方法最终返回的是一个BPANN的实例，我们可以直接使用这个实例来预测任何的有效的数据了。

下一节我会贴出整个的BPANN的代码。

[Prev]( 2.md) [Next]( 4.md )
### Python实现BP神经网络

我们会使用到Numpy这个数学库，但只是其中一些简单的函数，各位不要过分担心。另外我使用的Python的版本是2.7。

#### 初始化神经网络的参数

我们说过，神经网络至少包含三层——输入层、隐藏层、输出层，这些都是需要我们自己输入的数据，另外我们前面讲到的各个神经元的阈值与权值，但是却没有给出它们的具体的值，那是因为我们现在也不知道它们应该取什么值，所以我们就是用随机数来为这些取值与阈值来赋值吧。首先看看我们的整个的大框架：
```bash
#_*_coding:utf-8_*_
"""
Python实现BP神经网络
"""
#引入numpy
import numpy

class BPANN( object ):
	def __init__(self, sizes ):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [numpy.random.randn(y, 1) for y in self.sizes[1:]]
        self.weights = [numpy.random.randn(y, x)
                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]
    #
    # 还有其他的方法属性我们就先不列举了，先看看__init__函数吧
    #
```

`__init__`函数只有一个参数——`sizes`，我们规定这个参数是一个至少包含三个正整数的列表，比如`[100, 20, 10]`，这样的参数表明，我们需要定义的神经网络的输入层是100，隐藏层是20个神经元，输出层是10个神经元。在`__init__`函数的第一行我们定义一个属性来保存我们将要创建的神经网络的层数：
```bash
self.num_layers = len(sizes)
#如果按照我们刚才传递进去的[100, 20, 10]来计算的话，那么self.num_layers=3
```

接下来我们将要为整个神经网络的各个神经层的神经元初始化（随机化）它们的阈值与权值，想一想，我们说每一个神经元只有一个阈值，并且输入层的神经元是没有阈值的，所以我们只需要为隐藏层与输出层的神经元设置阈值即可，我们使用到了`numpy.random.randn( n,m )`这个函数，这个函数的作用是产生`n`行`m`列的随机数，看一下例子：

```bash
print numpy.random.randn( 3, 2 )
"""
[[ 0.35311067 -0.27524293]
 [ 0.81245797 -0.15912548]
 [ 0.79678259  0.9056547 ]]
"""
```
所以对于我们的阈值来说，假如我们的隐藏层有20个，输出层有10个，那么我们只需要一个20行1列以及10行1列的随机数即可，那么实现的语句就是：
```bash
self.biases = [numpy.random.randn(y, 1) for y in self.sizes[1:]]
```

对于权值来说，我们的神经网络的连接方式是全连接的，所以对于隐藏层来说，它前面与输入层的连接就有很多，还是拿上面的数据举一个例子，我们输入层有100，隐藏层有20个神经元，那么对于每一个隐藏层的神经元来说，它都需要与输入层的每一个神经元进行连接，那么一个神经元就有了100个权值，整个隐藏层的神经元的权值就有100*20个，那么实现的语句如下：

```bash
self.weights = [numpy.random.randn(y, x)
                for x, y in zip(self.sizes[:-1], self.sizes[1:])]
```

如果你对这行代码有一些疑问，那么你可以自己将其输出到控制台上看看，我来举一个例子：
```bash
net = BPANN( [2, 3, 2] )

print net.biases, net.weights

"""
输出如下：
	[array([[ 0.71505987],
       [-1.85425284],
       [-0.11128602]]), 
    	array([[ 0.24487705],
       [-0.17324133]])] 
	   #下面一部分是权值的部分
       [array([[-0.40839354,  0.88138576],
       [-0.86964437, -0.06575865],
       [ 1.19034414,  0.80310679]]), array([[ 0.53082191,  0.33469435,  0.0195962 ],
       [ 0.41325354, -1.17280611, -0.12307592]])]
"""
```

好了，神经网络的初始化已经完成了，现在我们想整理一下我们的训练数据集，假如说我们输入的训练的数据集有50000个，我们需要对每一个数据集求得它的成本函数的偏导数，然后进行累加，再最后除以50000，得到的偏导数是整个训练集的平均值，不过这样做代价有一些大，因为这样的话我们整个50000个样本就只能更新一次权值与阈值，所以我们要想办法将这50000个样本分割成不同的片段，每一个片段我们用来更新一下权值与阈值，这样我们的权值与阈值更新速度就会变得快一些。首先我们的样本是一堆元组组合成的列表，就像`[(x1,y1), (x2,y2), (x3,y3).......]`，每一个`x`都是一个包含所有的输入值的向量，每一个`y`都包含它所对应的输入值的期望输出，也是一个向量的形式，我们称之为标签（label）。在这个教程中，我们会使用神经网络来识别MNIST手写数字库里面的数字，所以我们过会儿会介绍如何从MNIST数据库中提取到数字，并将其组合成我们想要的向量的形式，先看一看我们怎么来分割这些给定的数据集：

```bash
    def SGD(self, training_data, epochs, mini_batch_size, eta,
            evaluation_data=None ):
        """
            SGD : （ stochastic gradient descent ）随机梯度下降的缩写，其实这个函数就是训练数据的函数。
            inputs: 
                training_data ，是一个包含多个输入与输出元组的列表，形如[(x1,y1), (x2,y2)···]
                epochs: 迭代次数
                mini_batch_size : 随机抽取样本的个数，对于一个有超大规模的输入集，我们将这些数据集分成多份来进行迭代更新权值以及阈值，该参数表示将当前训练集分割成多少份。
                eta : 学习速率
                evaluation_data : 用来验证当前网络的识别效率的数据集，如果有的话，在每一次迭代完成的时候都会显示当前的权值以及阈值下该神经网络的识别效率。
                
        """
                
        if evaluation_data is not None: 
            n_data = len(evaluation_data)
        n = len(training_data)
        
        for j in xrange(epochs):
            #打乱数据的排列，这是为了提高神经网络更好地学习，其实你不打乱这个顺序也是可以的
            random.shuffle(training_data)

            #开始对训练的数据进行切分，将其分割成多份，
            #具体的份数为len( trainData ) / a, a表示每一份的数据有多少个
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in xrange(0, n, mini_batch_size)]

            #刚才已经将训练的数据进行切分了，现在将每一组切分的传递到update_mini_batch
            #函数进行训练
            for mini_batch in mini_batches:
                self.update_mini_batch(
                    mini_batch, eta )

            #提示一下当前迭代的次数
            print "Epoch %s training complete" % j

            # 打印一下各个数据的预测百分比
            if evaluation_data is not None:
                accuracy = self.getAccuracy( evaluation_data )
                print "The accuracy of test data is %s / %s " % ( accuracy, n_data )
```

其实你会发现`SGD()`函数并没有涉及到我们前面提到的任何的公式，我们只是简单地将训练数据进行切分，并将其传递到`update_mini_batch`函数中，这个函数是我们马上要讲解的，不过在讲解之前我希望你能理解上面的代码。

篇幅太长对我的写作来说是一个不小的挑战，我觉得对你的阅读也是一个不小的挑战，所以我决定将`update_mini_batch`的讲解放在下一篇之中。

[Prev](../chapter2/5.md)    [Next]( 2.md )
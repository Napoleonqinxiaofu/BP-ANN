### 实现反向传播的算法1

什么都先别说，上代码先：

```bash
    def update_mini_batch(self, mini_batch, eta ):
        """
            更新权值与阈值的函数
            inputs : 
                mini_batch : 在SGD函数中将训练集分割成若干份之后的某一份
                eta : 学习速率

        """
        #
        #建立两个多维数组，让其与我们在开始的时候初始化的阈值（self.biases）
        #与权值（self.weights）有着相同的维度，并将其初始化为0
        nabla_b = [numpy.zeros(b.shape) for b in self.biases]
        nabla_w = [numpy.zeros(w.shape) for w in self.weights]
        
        for x, y in mini_batch:
        	#
            # 下面这一行代码比较关键，它是实现反向传播求出各个神经层的权值与阈值的偏差的关键函数
            #这个函数返回的是与self.weights、self.biases有着相同维度的多维数组，每一个值都是
            #相应的self.biases与self.weights在当前的训练输入x与期望输出y的梯度
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            
            #将在x输入以及期望输出y的学习下的权值与阈值的梯度与我们前面定义的多维数组
            #相加起来，这个相加是普通的矩阵相加，也就是对应的矩阵的元素相加
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
            
        #更新权值与阈值，使用我们前面讲过的梯度与偏导数之间关系的公式
        self.weights = [w-(eta/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb
                       for b, nb in zip(self.biases, nabla_b)]
```

在代码段中我尽量将注释写得多一些，希望你在看这些注释的时候不会感到困难。老实说我在学习这些知识的时候也是比较头疼的，因为第一遍根本不懂，所以我也没有狂妄到认为自己能写出来的东西是浅显易懂的，有一些隐涩，但是我相信你如果第一次没有看明白，第二遍或者是第三遍就会有一些感觉了，要不然咱们的祖先怎么会有“书读百遍，其义自现”这么经典的话呢？所以我希望正在阅读的你不要因为第一遍阅读这些文字的时候感到不适就放弃了，坚持一下。

首先在函数的开始我们定义了两个多维数组，分别是`nabla_b`和`nabla_w`，它们的维度与我们在`__init__`函数中定义的`self.biases`以及`self.weights`相同，我们要它们来干嘛呢？用来存放我们每一个训练数据的梯度的，到最后我们将其加起来，然后用来更新`self.biases`以及`self.weights`，现在你能明白为什么我们让它们的维度相等了吧？如果学习过numpy，你会发现numpy的矩阵操作非常方便，所以你会想在我们能不能将上面的代码进行简化一些呢？比如下面这句代码：

```bash
nabla_b = [numpy.zeros(b.shape) for b in self.biases]
```

我们可不可以写成：

```bash
nabla_b = numpy.zeros( self.biases.shape )
```

如果你这么干，你会发现自己的代码有毒——出错了，python告诉你self.biases没有shape的属性，这就对了，我们的`self.biases`是Python的普通的list，而不是numpy的数组，所以当然没有shape属性，所以我们还是乖乖使用最原始的方法来声明`nabla_b`吧。

接下来，我们创建了`nabla_b`和`nabla_w`，现在我们应该对每一个输入x与期望输出y计算权值与阈值的梯度了，所以我们使用到了`backprop`函数。你会发现，其实`update_mini_batch`这个函数只是计算了最终的权值与阈值，并更新，而没有涉及到实现我们前面的算法（推导的公式），这些实现都在`backprop`，马上就来。


### 反向传播算法实现部分2

```bash
    def backprop(self, x, y):
        """
            通过反向传播算法来求解在个输入数据下的各个神经层的偏差，进而求出它们各自的权值与阈值的偏差，并返回。
            inumpyuts :
                x : 某一个输入的数据
                y ： 期望输出的结果
            
            returns:
                返回求解得到的各个神经层的权值与阈值的偏差的列表
        """
        nabla_b = [numpy.zeros(b.shape) for b in self.biases]
        nabla_w = [numpy.zeros(w.shape) for w in self.weights]
        
        activation = x
        
        # 保存每一个神经层的激活值，x是一个类似向量的值
        activations = [x] 
        
        # 保存每一个神经层的z值，z = wx + b
        zs = []

        #计算输出层的激活值，这也是一个类似向量的东西
        for b, w in zip(self.biases, self.weights):
            z = numpy.dot(w, activation) + b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
            
        # 计算最后一层的偏差，进而才能计算前两层的偏差
        delta = activations[-1] - y
        nabla_b[-1] = delta
        nabla_w[-1] = numpy.dot( delta, activations[-2].transpose() )
        
        # 从倒数第二层开始计算各个神经层的偏差
        for l in xrange(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = numpy.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = numpy.dot(delta, activations[-l-1].transpose())
            
        return ( nabla_b, nabla_w )
```

在`backprop`函数的开始部分，我们也定义了`nabla_b`和`nabla_w`，最后我们会返回它们，你应该有一些联想，这两个多维数组应该在最后的时候保存的是当前的训练值x，y之下的权值与阈值的梯度。其实这也没有什么好解释的，如果你对照着前面我们推导的公式，就能看明白这些公式。对着解释这些东西，我感到无能为力，希望大家原谅，因为我个人觉得解释代码这种东西首先要知道我们遇到的是什么问题，到这里，我只能祈祷你能仔细地对照着前面推导的公式来查看了。

好了，推导与实现的部分都讲完了，后面的一些东西是修饰，但是我相信这些东西值得一看！

[Prev]( 1.md )  [Next]( 3.md)
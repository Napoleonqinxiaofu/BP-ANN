### 反向传播

前面我们已经介绍成本函数关于权值与阈值的偏导数的求解方法，并且已经给出了公式。你可以发现在两个偏导数的公式中都有一个式子——`y-a(z)`，这玩意是什么东西？我们在前面定义过，`y`表示某一层神经元的在某一个数据输入下的期望输出，而`a(z)`是实际输出。这就有点儿是当前神经层的一个误差了，虽然我们在介绍成本函数的时候说误差一把要平方一下，但是刚才的式子其实也是一种误差，不是吗？不过现在我们不会再纠结于这个东西了，我们要说的是神经网络如何使用这些参数的。

对于一组训练的数据，我们知道了输入是多少，期望的输出是多少，如果没有什么反馈，也就是只运行一遍的那种，那么我们也会知道实际的最终输出是多少，现在我们重新定义一下一个神经元的误差（好像咱们这是第三次定义它的误差形式了啊！！！）不过这个误差不再是简单的期望输出与实际输出的误差，而是一种变化量。

想象一下，我们的成本函数是关于`z`的函数，那么我们假设我们改动一下`z`的值，那么成本函数应该是会发生改变的，现在我们定义这种因为改变`z`而成本函数发生改变的该变量称之为一种误差，对于一个神经元是这样的，那么我们也可以想象对于整个一个的神经层也应该是一样的，我们还是像前面一样使用向量的方式来表示整个神经层的这种误差，对于最后一层，我们使用下面的方式来表示：
![chapter2_5md_delta_of_layer]( ../image/chapter2/chapter2_5md_delta_of_layer.png )